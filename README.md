# scrapper-deportes-devops
# Scraper de Posiciones de la Premier League con CI/CD

Este proyecto implementa un scraper automatizado para extraer las tablas de posiciones de la Premier League desde [ESPN.com.co](https://www.espn.com.co/futbol/posiciones/_/liga/eng.1). Utiliza un robusto pipeline de DevOps con **GitHub Actions** para asegurar la calidad del cÃ³digo, la ejecuciÃ³n automatizada y el despliegue continuo de los datos.

## **DescripciÃ³n General**

El scraper estÃ¡ diseÃ±ado para obtener informaciÃ³n clave de la Premier League, incluyendo:

* **Detalles del Equipo**: PosiciÃ³n en la tabla, abreviatura y nombre completo del equipo.
* **EstadÃ­sticas de Partidos**: NÃºmero de partidos jugados, ganados, empatados y perdidos.
* **Detalles de Goles**: Goles a favor, goles en contra y diferencia de puntos.
* **Puntos Totales**.

El objetivo principal es proporcionar datos actualizados y confiables de la liga de fÃºtbol mÃ¡s competitiva, garantizando la fiabilidad a travÃ©s de pruebas automatizadas y la entrega continua de datos a travÃ©s de GitHub Actions.

## **TecnologÃ­as Utilizadas**

* **Python**: Lenguaje de programaciÃ³n principal.
* **`requests`**: Para realizar solicitudes HTTP a la pÃ¡gina web.
* **`BeautifulSoup4`**: Para parsear el HTML y extraer los datos estructurados.
* **`pandas`**: Para la manipulaciÃ³n, organizaciÃ³n y almacenamiento de datos en DataFrames (finalmente guardados en CSV).
* **Git**: Sistema de control de versiones para gestionar el historial del cÃ³digo.
* **GitHub**: Plataforma para alojar el repositorio, facilitar la colaboraciÃ³n y configurar GitHub Actions.
* **GitHub Actions**: Motor de automatizaciÃ³n para el pipeline de IntegraciÃ³n Continua (CI) y Despliegue Continuo (CD).
* **`pytest`**: Framework de pruebas para escribir y ejecutar pruebas unitarias automatizadas.

## **Estructura del Repositorio**

scraper-deportes-devops/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Plazas__Andres_Analizando_EA1.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ scraper.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_scraper.py
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yml
â”œâ”€â”€ data/
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

### Prerrequisitos

- Docker & Docker Compose
- Git
- Python 3.9+ (para desarrollo local)

### InstalaciÃ³n RÃ¡pida

```bash

# 1. Clonar el repositorio

git clone https://github.com/AndresPlazas19931504/scrapper-deportes-devops.git
cd scrapper-deportes-devops

# 2. IMPORTANTE: Configurar credenciales personales

cp .env.example .env

# Editar .env con tus credenciales personales

# 3. Construir y ejecutar

make build
make up
```

## ğŸ” ConfiguraciÃ³n de Credenciales: Personalizar Credenciales

**Antes de usar este proyecto, DEBES personalizar las siguientes credenciales en el archivo `.env`:**

```bash

# Copia el archivo de ejemplo
cp .env.example .env

# Edita el archivo .env y cambia:

SCRAPER_USER=Su_usuario_aqui
EMAIL_FROM=Su_email@dominio.com
EMAIL_PASSWORD=Su_contraseÃ±a_de_aplicacion
GITHUB_TOKEN=Su_token_de_github__personal
API_KEY=Su_api_key_persona
SECRET_KEY=Su_Key_Secreta

```
## Base de Datos SQLite3

Este proyecto utiliza **SQLite3** como base de datos principal:

```
ğŸ“‚ db/
â”œâ”€â”€ deportes_andres.db
â”œâ”€â”€ backups/
â””â”€â”€ schema.sql
```

## Comandos Disponibles

```bash

# ConstrucciÃ³n y ejecuciÃ³n

make build      # Construir imagen Docker
make up         # Levantar servicios
make down       # Detener servicios
make run        # Ejecutar solo el scraper

# Desarrollo

make logs       # Ver logs en tiempo real
make shell      # Acceso shell del contenedor
make test       # Ejecutar tests
make clean      # Limpiar contenedores e imÃ¡genes

# Utilidades
make rebuild    # Reconstruir sin cache

```

## Pipeline CI/CD

### GitHub Actions AutomÃ¡tico

El pipeline se ejecuta automÃ¡ticamente cuando:
- Push a rama `main`
- Pull Request a `main`
- Push a rama `develop`

### Etapas del Pipeline:

1. ** Tests** - Ejecuta pytest con coverage
2. ** Build** - Construye imagen Docker
3. ** Push** - Sube a GitHub Container Registry
4. ** Deploy** - Despliegue automÃ¡tico

## Estructura del Proyecto

scraper-deportes-devops/

â”œâ”€â”€ Dockerfile                 # Imagen Docker optimizada
â”œâ”€â”€ docker-compose.yml         # OrquestaciÃ³n
â”œâ”€â”€ Makefile                   # Comandos automatizados
â”œâ”€â”€ .env                       # ConfiguraciÃ³n personal
â”œâ”€â”€ requirements.txt           # Dependencias Python
â”œâ”€â”€ .github/workflows/         # Pipeline CI/CD
â”œâ”€â”€ src/                       # CÃ³digo fuente
â”œâ”€â”€ tests/                     # Tests automatizados
â”œâ”€â”€ db/                        # Base datos SQLite
â””â”€â”€ data/                      # Datos procesados
```

## Desarrollo Local

### ConfiguraciÃ³n del Entorno

```bash

# 1. Crear entorno virtual

venv\Scripts\activate     # Windows

# 2. Instalar dependencias

pip install -r requirements.txt

# 3. Configurar BD SQLite

python src/database.py --init

# 4. Ejecutar tests

pytest tests/ -v
```
## Docker

### Imagen Optimizada

```dockerfile

# CaracterÃ­sticas:

- Python 3.9 slim
- SQLite3 integrado
- Dependencias mÃ­nimas
- Usuario no-root
- VolÃºmenes persistentes

```

### VolÃºmenes Importantes

```yaml

volumes:
  - ./db:/app/db          # Base datos persistente
  - ./logs:/app/logs      # Logs persistentes
  - ./data:/app/data      # Datos procesados

```

## Monitoreo

### Logs Disponibles

```bash
# Ver todos los logs

make logs

# Logs especÃ­ficos

docker-compose logs scraper-deportes-andres

# Logs de errores

tail -f logs/error.log
```

### Health Check

```bash
# Verificar estado del contenedor

docker-compose ps

# Ejecutar health check manual

python scripts/health_check.py
```
